{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3748b00f-2794-4e25-8b0a-0bc732d9dda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "006bbb7c-ba84-4261-a969-ef807f7ac5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init('/home/gerardo-rodriguez/spark-4.0.0-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "736d22c8-d1e4-41a9-8f25-23833d9ccc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b32d47-3a84-4582-bc74-cfb03e3beeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/19 10:50:52 WARN Utils: Your hostname, Lanz-Lenovo, resolves to a loopback address: 127.0.1.1; using 192.168.1.145 instead (on interface wlp2s0)\n",
      "25/08/19 10:50:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/19 10:50:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/19 10:50:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('logreg').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "125046d3-150c-4368-bd8e-31b6578a8c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = spark.read.csv('titanic.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "198ee67f-e8b2-44eb-bde1-38beda3c5dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00a5926b-ad84-40b2-bbdd-898a509d4785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cec5534b-3087-4a5f-ae3a-d1d778f6abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cols = my_data.select(['Survived',\n",
    "                    'Pclass',\n",
    "                    'Sex',\n",
    "                    'Age',\n",
    "                    'SibSp',\n",
    "                    'Parch',                \n",
    "                    'Fare',\n",
    "                    'Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad6e33c4-123a-41e6-8e97-ab4daac2bc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_cols.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17ed57da-281d-4f03-9dac-6a3abd10fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_final_data = my_cols.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db8a96c0-5fe8-4b0e-bebc-75e8b2acc60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler, VectorIndexer,\n",
    "                                OneHotEncoder, StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a0e97b6a-cde8-487b-9eff-5570107600d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_indexer = StringIndexer(inputCol='Sex', outputCol='SexIndex')\n",
    "gender_encoder = OneHotEncoder(inputCol='SexIndex', outputCol='SexVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15ec30e9-a051-43b6-b049-9b5e2544c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "embark_indexer = StringIndexer(inputCol='Embarked', outputCol='EmbarkIndex')\n",
    "embark_encoder = OneHotEncoder(inputCol='EmbarkIndex', outputCol='EmbarkVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9c686dd3-6450-49ff-81c9-5dfe57bc7769",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Pclass', 'SexVec', 'EmbarkVec', 'Age', 'Sibsp', 'Parch', 'Fare'],\n",
    "                           outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8f66831-c13f-467c-8446-75a6e368c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e18619e-55e3-482e-a0f0-d783bf12724f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da1aeff8-2d39-4e5d-9529-05c7b69022f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_titanic = LogisticRegression(featuresCol='features', labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7456f734-8566-4e4d-b65e-571686c02a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[gender_indexer, embark_indexer, gender_encoder, \n",
    "                            embark_encoder, assembler, log_reg_titanic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f3c0193-2f7e-4d27-81af-3584a23e5c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = my_final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "314d869c-a3d0-4f7c-a2d8-4d2bab8a1935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/19 11:10:17 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "fit_model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e6f3bcd3-1243-4689-bf66-12f47b93f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fit_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0abfe8fc-5814-4f9b-8f08-4d501da5fe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "064cf868-2b1f-40ce-a900-b94379e77a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3c4a59a-0df2-44a7-8c8c-c45f36144a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|Survived|prediction|\n",
      "+--------+----------+\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "results.select('Survived', 'prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd7f7769-2452-44d4-817a-7505d467e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC = my_eval.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c815165e-eada-4d61-bfd2-16be67ccdc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8280430457587268"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
