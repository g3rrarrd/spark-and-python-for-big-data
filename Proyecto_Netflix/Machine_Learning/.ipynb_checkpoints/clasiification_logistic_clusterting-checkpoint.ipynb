{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44628c9-f5a9-45f0-9c53-0ee744b73f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/gerardo-rodriguez/spark-4.0.0-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63deac61-c531-4843-8c84-e4dea2c3169d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/27 10:14:41 WARN Utils: Your hostname, Lanz-Lenovo, resolves to a loopback address: 127.0.1.1; using 192.168.1.145 instead (on interface wlp2s0)\n",
      "25/08/27 10:14:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/27 10:14:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/27 10:14:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ml').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3bb4856-75d1-4d27-9d04-5a4a41606370",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../Create_ratings/netflix_users.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c75a3868-0096-44e9-a3bb-0cadf3eb27b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Subscription_Type: string (nullable = true)\n",
      " |-- Watch_Time_Hours: double (nullable = true)\n",
      " |-- Favorite_Genre: string (nullable = true)\n",
      " |-- Last_Login: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf97009-3a4f-417e-95e8-dbfe5a63b702",
   "metadata": {},
   "source": [
    "# Create Column Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "472cc3e9-8c76-476c-9bed-60ad75aea883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, datediff, current_date, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28f1af2b-48ff-4c73-81f6-7408c030eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_churn = df.withColumn('churn', when((col('Watch_Time_Hours') < 300) & (datediff(current_date(), col('Last_Login')) > 60), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b604dc6-3f16-4bf8-846f-83844818fe4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+---+-------+-----------------+----------------+--------------+----------+-----+----------+\n",
      "|User_ID|          Name|Age|Country|Subscription_Type|Watch_Time_Hours|Favorite_Genre|Last_Login|churn|days_login|\n",
      "+-------+--------------+---+-------+-----------------+----------------+--------------+----------+-----+----------+\n",
      "|      1|James Martinez| 18| France|          Premium|           80.26|         Drama|2024-05-12|    1|       472|\n",
      "|      2|   John Miller| 23|    USA|          Premium|          321.75|        Sci-Fi|2025-02-05|    0|       203|\n",
      "|      3|    Emma Davis| 60|     UK|            Basic|           35.89|        Comedy|2025-01-24|    1|       215|\n",
      "|      4|   Emma Miller| 44|    USA|          Premium|          261.56|   Documentary|2024-03-25|    1|       520|\n",
      "|      5|    Jane Smith| 68|    USA|         Standard|           909.3|         Drama|2025-01-14|    0|       225|\n",
      "+-------+--------------+---+-------+-----------------+----------------+--------------+----------+-----+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_ml = df_churn.withColumn('days_login', datediff(current_date(), col('Last_Login')))\n",
    "df_ml.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace095b1-cb14-4c1c-be90-bbb72fd05631",
   "metadata": {},
   "source": [
    "# Classification Model for churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b90ef71b-e11b-4c74-9ce1-72efa46ad3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import (LogisticRegression, RandomForestClassifier)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cfd7494-efd5-43a4-aa37-04165529c944",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCols=['Subscription_Type', 'Country'], outputCols=['subscription_indexed', 'country_indexed'])\n",
    "indexed = indexer.fit(df_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4897d07d-1b1e-4729-83ea-4a1e94333349",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexed = indexed.transform(df_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6608a6b5-4921-43b9-9c78-ba204c4b0e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['User_ID',\n",
       " 'Name',\n",
       " 'Age',\n",
       " 'Country',\n",
       " 'Subscription_Type',\n",
       " 'Watch_Time_Hours',\n",
       " 'Favorite_Genre',\n",
       " 'Last_Login',\n",
       " 'churn',\n",
       " 'days_login',\n",
       " 'subscription_indexed',\n",
       " 'country_indexed']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_indexed.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c3bb21-0818-4777-ace9-76778a66302a",
   "metadata": {},
   "source": [
    "## Seleccion de variables \n",
    "\n",
    "Para la predicción de abandono (churn), se seleccionaron las siguientes variables por su relevancia potencial en el comportamiento del usuario:\n",
    "\n",
    "Edad (Age): Los usuarios más jóvenes, al estar más familiarizados con la tecnología y con mayor disposición a explorar nuevas plataformas de streaming, presentan una mayor probabilidad de migrar. En contraste, los usuarios de mayor edad tienden a mantener hábitos más estables, lo que podría reflejarse en una menor tasa de abandono.\n",
    "\n",
    "Horas de visualización (Watch_Time_Hours): Un mayor tiempo de visualización es un fuerte indicador de compromiso con la plataforma. Si el usuario encuentra contenido de interés y dedica muchas horas, la probabilidad de abandono disminuye significativamente.\n",
    "\n",
    "Días desde el último acceso (Days_Login): La inactividad prolongada es una señal directa de desinterés o desconexión con el servicio. Usuarios que no han ingresado recientemente son más propensos a cancelar su suscripción.\n",
    "\n",
    "Tipo de suscripción (Subscription_Type): Los usuarios con planes de mayor costo (como Premium) suelen compartir la cuenta con familiares o amigos. Este factor reduce la probabilidad de abandono, ya que la decisión involucra a más personas. En cambio, los planes básicos, generalmente individuales, presentan mayor volatilidad.\n",
    "\n",
    "País (Country): La situación económica de cada país influye en la capacidad de pago. En regiones donde el valor del dólar representa un costo elevado, los usuarios pueden considerar la suscripción como un gasto prescindible, aumentando el riesgo de abandono.\n",
    "\n",
    "## Features Selection\n",
    "\n",
    "For churn prediction, the following variables were selected due to their potential relevance to user behavior:\n",
    "\n",
    "Age: Younger users, being more familiar with technology and more inclined to explore alternative streaming platforms, may have a higher probability of leaving. In contrast, older users often maintain more stable habits, which may translate into lower churn rates.\n",
    "\n",
    "Watch Time (Watch_Time_Hours): Higher watch time is a strong indicator of user engagement. If users find content of interest and dedicate significant hours to watching, the likelihood of churn decreases.\n",
    "\n",
    "Days Since Last Login (Days_Login): Extended inactivity is a direct signal of disengagement. Users who have not logged in recently are more likely to cancel their subscription.\n",
    "\n",
    "Subscription Type (Subscription_Type): Users with higher-tier plans (e.g., Premium) often share accounts with family or friends. This shared dependency lowers the probability of churn, as the decision to cancel involves multiple people. Conversely, basic plans, usually for individual use, tend to be more volatile.\n",
    "\n",
    "Country: Economic conditions in each country affect users’ ability to pay. In regions where the dollar represents a relatively high cost, users may perceive the subscription as a non-essential expense, increasing churn risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5995babe-a62b-4c46-bdf2-2921112670be",
   "metadata": {},
   "outputs": [],
   "source": [
    "asembler = VectorAssembler(inputCols=['Age',\n",
    " 'Watch_Time_Hours',\n",
    " 'days_login',\n",
    " 'subscription_indexed',\n",
    " 'country_indexed'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23e55393-8248-4e8e-a697-cffeaf27b976",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembled = asembler.transform(df_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83ea3e0b-0f66-4f82-b6b0-739b653b63f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Subscription_Type: string (nullable = true)\n",
      " |-- Watch_Time_Hours: double (nullable = true)\n",
      " |-- Favorite_Genre: string (nullable = true)\n",
      " |-- Last_Login: date (nullable = true)\n",
      " |-- churn: integer (nullable = false)\n",
      " |-- days_login: integer (nullable = true)\n",
      " |-- subscription_indexed: double (nullable = false)\n",
      " |-- country_indexed: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_assembled.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a43ea2a3-e98d-4f2c-8634-939a3cd474ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df_assembled.randomSplit([0.8, 0.2], seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c21dc-4e1e-42fc-907d-9c9a92e050ad",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71934f1f-2496-4c1d-bf13-b8685aac91ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/27 10:14:53 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "logistic = LogisticRegression(featuresCol='features', labelCol='churn')\n",
    "model = logistic.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0c4b991-b232-46a7-84c9-d2fe80d6fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75799ff6-b06a-4a57-979e-f3dbe9877ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|churn|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "prediction.select(['churn', 'prediction']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58c9b495-2789-46bc-86f8-ff581eaff590",
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_log = BinaryClassificationEvaluator(labelCol='churn')\n",
    "pred_log = eva_log.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c96c016-585d-49b0-9e98-b4b48d90b563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Logistic regression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.999999415457866"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Evaluation Logistic regression\")\n",
    "pred_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300f1d9c-a588-4dc4-aa72-6e11ac46ed10",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "744767eb-6b67-4117-a276-8e9f6750a0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/27 10:15:08 WARN DAGScheduler: Broadcasting large task binary with size 1056.2 KiB\n",
      "25/08/27 10:15:09 WARN DAGScheduler: Broadcasting large task binary with size 1535.9 KiB\n",
      "25/08/27 10:15:10 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/08/27 10:15:11 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/08/27 10:15:12 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/08/27 10:15:14 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/08/27 10:15:15 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "25/08/27 10:15:16 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/08/27 10:15:17 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/08/27 10:15:18 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/08/27 10:15:19 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/08/27 10:15:20 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/08/27 10:15:21 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/08/27 10:15:21 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(featuresCol='features', labelCol='churn', maxDepth=20, numTrees=200, seed=1)\n",
    "model_rf = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f79530d7-a2e9-429a-b9f1-156276b264cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/27 10:15:23 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|churn|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "predict_rf = model_rf.transform(test_data)\n",
    "predict_rf.select(['churn', 'prediction']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e33dc7c-b94b-4739-afdd-14857e2ea4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/27 10:15:24 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/08/27 10:15:24 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/08/27 10:15:25 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/08/27 10:15:26 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n"
     ]
    }
   ],
   "source": [
    "eva_accuracy = MulticlassClassificationEvaluator(labelCol='churn', metricName='accuracy')\n",
    "eva_precision = MulticlassClassificationEvaluator(labelCol='churn', metricName='weightedPrecision')\n",
    "eva_recall = MulticlassClassificationEvaluator(labelCol='churn', metricName='weightedRecall')\n",
    "eva_f1 = MulticlassClassificationEvaluator(labelCol='churn', metricName='f1')\n",
    "\n",
    "accuracy = eva_accuracy.evaluate(predict_rf)\n",
    "precision = eva_precision.evaluate(predict_rf)\n",
    "recall = eva_recall.evaluate(predict_rf)\n",
    "f1 = eva_f1.evaluate(predict_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ad11ca4-b04f-4f9a-8fd9-72ff76b2395f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas Random Forest\n",
      "Accuracy : 0.9954\n",
      "Precision: 0.9955\n",
      "Recall   : 0.9954\n",
      "F1 Score : 0.9954\n"
     ]
    }
   ],
   "source": [
    "print(\"Métricas Random Forest\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall   : {recall:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4510f4ad-f0fe-4347-af7c-b0150954cc14",
   "metadata": {},
   "source": [
    "## Evaluation Metrics – Random Forest (Churn Prediction)\n",
    "\n",
    "| Metric     | Value  |\n",
    "|------------|--------|\n",
    "| Accuracy   | 0.9954 |\n",
    "| Precision  | 0.9960 |\n",
    "| Recall     | 0.9940 |\n",
    "| F1 Score   | 0.9950 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d668c50f-98d6-4c7f-bfc1-cc7994d4efed",
   "metadata": {},
   "source": [
    "# Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0136e342-3ca2-43d9-b044-6c8a9b68994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9a4c8149-a1d0-40b1-9fd9-4e4e2894f25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_show = spark.read.csv('../Create_ratings/netflix_titles.csv', header=True, inferSchema=True, quote='\"', escape='\"')\n",
    "df_show = df_show.select(['show_id',\n",
    "                     'type',\n",
    "                     'title',\n",
    "                     'director',\n",
    "                     'country',\n",
    "                     'date_added',\n",
    "                     'release_year',\n",
    "                     'rating',\n",
    "                     'duration',\n",
    "                     'listed_in',\n",
    "                     'description']).filter(df_show['type'] == 'Movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "200d5631-7c11-424e-bff1-2ec29d175b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eda0bf68-c5d5-425b-b66e-6c91ab3c6126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_show = df_show.withColumn('rate', rand() * 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2824110e-0854-4acb-93ee-c19ecf215473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- show_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- director: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- date_added: string (nullable = true)\n",
      " |-- release_year: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- listed_in: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- rate: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_show = df_show.dropna('any')\n",
    "df_show.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "50540972-fe54-46e0-a950-1757d6c143cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['show_id',\n",
       " 'type',\n",
       " 'title',\n",
       " 'director',\n",
       " 'country',\n",
       " 'date_added',\n",
       " 'release_year',\n",
       " 'rating',\n",
       " 'duration',\n",
       " 'listed_in',\n",
       " 'description',\n",
       " 'rate']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_show.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0b4283b6-1888-4e5d-b4f3-47537684e1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- director: string (nullable = true)\n",
      " |-- release_year: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- rate: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = df_show.select([\n",
    "    'title',\n",
    " 'director',\n",
    " 'release_year',\n",
    " 'duration',\n",
    " 'rating',\n",
    " 'rate'])\n",
    "\n",
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ad0dceff-fc00-4ff2-aaa6-4fe755418c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_director = StringIndexer(inputCol='director', outputCol='director_indexed')\n",
    "indexer_rating = StringIndexer(inputCol=\"rating\", outputCol=\"rating_indexed\", handleInvalid=\"keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "75d5f2bd-d968-4960-b468-6938a2a1040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, day, regexp_replace, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "282ed23d-596e-43e3-88d2-8da8c2ed336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = df.withColumn(\"year\", col(\"release_year\").cast('int'))\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"duration_num\",\n",
    "        regexp_replace(col(\"duration\"), \" min\", \"\").cast(\"int\")\n",
    "    )\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a4819dc5-2e45-434b-983b-1888dfe9ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepared = preprocess(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b0673b78-ce01-429f-9720-f943f07c7e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_rf = VectorAssembler(inputCols=[ 'year', 'duration_num','rating_indexed', 'director_indexed'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9e912c73-62f7-4f41-abf0-8fb68e1b511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression(featuresCol='features', labelCol='rate', maxIter=150, regParam=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "714703f9-9fca-48f2-b831-b86fb3573a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b8711ed1-f33a-42cc-9684-e59408e696e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[indexer_director, indexer_rating, assembler_rf, lin_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cf990e75-b562-4af6-9918-af2ed6d05adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "484b4889-6a61-407e-8283-fbccc7d8d7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+\n",
      "|title               |rate                |prediction         |\n",
      "+--------------------+--------------------+-------------------+\n",
      "|Dick Johnson Is Dead|0.058322911102679176|0.4982403671710855 |\n",
      "|Sankofa             |0.6532677923170594  |0.49794850359588194|\n",
      "|The Starling        |0.9694531196177445  |0.49821352435614297|\n",
      "|Je Suis Karl        |0.4238528893905017  |0.4981699414748908 |\n",
      "|Jeans               |0.6003057609903206  |0.4978976581005633 |\n",
      "|Grown Ups           |0.9952457092022428  |0.498143409843807  |\n",
      "|Dark Skies          |0.35172675562028033 |0.4981774360612422 |\n",
      "|Paranoia            |0.8658451212854049  |0.49816129908150336|\n",
      "|Birth of the Dragon |0.41232966044634056 |0.49820665464380065|\n",
      "|Jaws                |0.7694839742121337  |0.4978221170266708 |\n",
      "+--------------------+--------------------+-------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(df_prepared)\n",
    "predictions.select(\"title\", \"rate\", \"prediction\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "33dde9cb-9a80-44ac-b17d-0f1ae47d64aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c936b5-4d01-43fa-a0bc-e07b49d7897d",
   "metadata": {},
   "source": [
    "## Nota sobre la calidad del modelo\n",
    "\n",
    "Es importante recalcar que las métricas de evaluación (RMSE, MAE y R²) presentan valores bajos debido a que los datos utilizados fueron generados de manera completamente aleatoria. Esto significa que no existe una relación lógica o causal entre las variables predictoras y la variable objetivo (rate). En consecuencia, el modelo no puede encontrar patrones reales y sus predicciones no son representativas de un escenario práctico. El objetivo de este ejercicio es únicamente demostrar la aplicación de técnicas de Machine Learning con PySpark y no obtener un modelo predictivo útil en la realidad.\n",
    "\n",
    "It is important to highlight that the evaluation metrics (RMSE, MAE, and R²) show poor values because the dataset was entirely randomly generated. This means that there is no logical or causal relationship between the predictor variables and the target variable (rate). As a result, the model cannot identify real patterns, and its predictions are not representative of a practical scenario. The purpose of this exercise is solely to demonstrate the application of Machine Learning techniques with PySpark, rather than to build a useful predictive model in reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "462ccf61-c33c-4341-bcdb-eafb06f7e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_rmse = RegressionEvaluator(labelCol=\"rate\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator_r2 = RegressionEvaluator(labelCol=\"rate\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "evaluator_mae = RegressionEvaluator(labelCol=\"rate\", predictionCol=\"prediction\", metricName=\"mae\")\n",
    "\n",
    "rmse = evaluator_rmse.evaluate(predictions)\n",
    "r2 = evaluator_r2.evaluate(predictions)\n",
    "mae = evaluator_mae.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5ec1f250-ecb1-45a8-8389-d7c28a27ffe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de Evaluación:\n",
      "RMSE: 0.2875\n",
      "MAE:  0.2479\n",
      "R²:   0.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"Métricas de Evaluación:\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"R²:   {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1048a196-f8e5-476b-8a34-6d0aaddc931c",
   "metadata": {},
   "source": [
    "# Evaluation Metrics – Linear Regression\n",
    "\n",
    "| Metric     | Value  |\n",
    "|------------|--------|\n",
    "| RMSE   | 0.2912 |\n",
    "| r2  | 0.2530 |\n",
    "| mae     | 0.0000 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a5ee0-321d-4b20-84ad-44d2af2579c5",
   "metadata": {},
   "source": [
    "# Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ce995-8eb3-4b2b-90db-80e576e45ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d76323-b1ab-4b43-a39d-43ceb4a4b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = spark.read.csv('../Create_ratings/netflix_users.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2454741-b60d-4301-81b5-e7406361f7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a17d430-dd97-40e2-ac6c-ecb931f63438",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9bf0c9-3217-444d-a00a-448c678942f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer_country = StringIndexer(inputCol='Country', outputCol='country_indexed')\n",
    "indexer_sub = StringIndexer(inputCol='Subscription_Type', outputCol='sub_indexed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a138b4-5e6f-4bdb-90ce-69b991aefe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler_k = VectorAssembler(inputCols=[\n",
    "'country_indexed',\n",
    " 'Watch_Time_Hours',\n",
    " ], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d51fc5-9856-4e65-bd3c-964dd3ed5807",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler(inputCol='features', outputCol='features_scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1f6eb4-154f-49fe-a934-d5b27a05a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = [x for x in range(2,11)]\n",
    "\n",
    "for i in val:\n",
    "    k_mean = KMeans(k=i, featuresCol='features_scale', seed=1)\n",
    "    pipeline_km = Pipeline(stages=[indexer_country, indexer_sub, assembler_k, scale, k_mean])\n",
    "    model = pipeline_km.fit(df_users)\n",
    "    predic = model.transform(df_users)\n",
    "    eva_clu = ClusteringEvaluator(featuresCol='features_scale')\n",
    "\n",
    "    print(f'prediction with k = {i}')\n",
    "    print(eva_clu.evaluate(predic))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5e40d2-c4c5-47f5-aa32-6e28d2793c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_mean = KMeans(k=4, featuresCol='features_scale', seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31357a80-d137-4508-8fc0-3530f2b50f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_km = Pipeline(stages=[indexer_country, indexer_sub, assembler_k, scale, k_mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6197756a-34c9-446e-8243-88b1c113f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_k = pipeline_km.fit(df_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260ee38b-fed4-4318-8bf5-ff67163bef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_k = model_k.transform(df_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb8285d-e908-489e-a4bf-615095285e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_k.select('Age', 'Watch_Time_Hours', 'prediction').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6c055e-1db1-4b62-beee-7a6e5a7dd28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078df651-2df6-4c9c-9a80-80ca5be1ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_clu = ClusteringEvaluator(featuresCol='features_scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67cfc6c-1c32-48f9-a2d8-d6830256d377",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('eva of kmean 3')\n",
    "eva_clu.evaluate(prediction_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557330e2-c95d-4cc0-8271-e3c7d71096c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650dabad-082e-427b-bc04-ad0c625d1e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_k.groupBy('prediction').agg(\n",
    "    avg('Age'),\n",
    "    avg('Watch_Time_Hours')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3cf95-65c1-486e-8cca-93e83518ba2e",
   "metadata": {},
   "source": [
    "# Evaluation Groups – Kmeans\n",
    "\n",
    "\n",
    "|prediction|          avg(Age)|avg(Watch_Time_Hours)|\n",
    "|----------|------------------|---------------------|\n",
    "|         1| 46.54018632559608|    745.0765687667777|\n",
    "|         3| 46.70972471086496|   255.25532008470475|\n",
    "|         2| 46.51924005796168|    748.6533633875353|\n",
    "|         0|46.169225898369476|   249.52558018046565|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
