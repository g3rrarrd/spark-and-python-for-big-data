{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e180c668-d99d-4952-947c-34e7a7dd0372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/gerardo-rodriguez/spark-4.0.0-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1c029a5-db4f-4aa2-8bdc-7b8e09a7c65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/27 10:54:57 WARN Utils: Your hostname, Lanz-Lenovo, resolves to a loopback address: 127.0.1.1; using 192.168.1.145 instead (on interface wlp2s0)\n",
      "25/08/27 10:54:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/27 10:54:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('nlp').config('spark.executor.memory', '4g').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c463df4f-f5bf-4003-ba1b-a669937e1baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('IMDB_Dataset.csv', header=True, inferSchema=True, quote='\"', escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b71207-c89b-4100-87aa-1c6bdbf00913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review: string (nullable = true)\n",
      " |-- sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9466d8e2-11c7-4ac9-8168-2c4cbc199f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|              review|sentiment|\n",
      "+--------------------+---------+\n",
      "|One of the other ...| positive|\n",
      "|A wonderful littl...| positive|\n",
      "|I thought this wa...| positive|\n",
      "|Basically there's...| negative|\n",
      "+--------------------+---------+\n",
      "only showing top 4 rows\n"
     ]
    }
   ],
   "source": [
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0dbf8cd-05ed-456f-b715-e6f2ee703489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length, col, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "882622e7-ba9a-480a-9224-3df657c7f05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('len_review', length(col('review')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfb4ebc9-55cf-4c71-a209-93da95cf4fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:==============>                                           (3 + 9) / 12]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+\n",
      "|sentiment|avg(len_review)|\n",
      "+---------+---------------+\n",
      "| positive|     1324.79768|\n",
      "| negative|     1294.06436|\n",
      "+---------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.groupBy('sentiment').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24df4340-1cf3-4ebc-b874-8cc457681ee3",
   "metadata": {},
   "source": [
    "## token, reg_roke, stop_remove, count_vec, idf, Indexer, Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8073113e-b4d4-454e-9f4e-732d41dfa2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, StringIndexer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.sql.functions import col, udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "709a5712-fe4d-4c74-8378-befaf259e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_token = RegexTokenizer(inputCol='review', outputCol='words', pattern='\\\\W+')\n",
    "stop_remove = StopWordsRemover(inputCol='words', outputCol='stop_token')\n",
    "count_vec = CountVectorizer(inputCol='stop_token', outputCol='count_token')\n",
    "idf = IDF(inputCol='count_token', outputCol='tk_idf')\n",
    "pos_neg = StringIndexer(inputCol='sentiment', outputCol='sentiment_indexed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd75bb7b-474b-4bb4-bc5e-7bc14f0c9d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa6bd8e6-231c-4bca-b5c9-0653d923f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = VectorAssembler(inputCols=['tk_idf', 'len_review'], outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fe62f8-29d0-4f36-8bb0-83af72af4b1d",
   "metadata": {},
   "source": [
    "# Use Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d502c087-167c-489f-b6d3-ca554b12a272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba8544d2-a6b9-4809-bc34-be3266bc0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(featuresCol='features' ,labelCol='sentiment_indexed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ea4318a-a3de-4f3f-b804-808b9a4e85eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "144ffb03-dd1c-4cce-99f8-ae83f0f6043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[reg_token, stop_remove, count_vec, idf, pos_neg, clean, log_reg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2cb9a33-f006-4c5d-8b01-adf9cec9f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab7bf2-6d5f-4d99-b855-0d794e81392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25d604f7-2f57-4d7d-a546-3f118d05a9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/27 10:59:19 WARN DAGScheduler: Broadcasting large task binary with size 6.7 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+\n",
      "|sentiment_indexed|prediction|\n",
      "+-----------------+----------+\n",
      "|              1.0|       0.0|\n",
      "|              1.0|       1.0|\n",
      "|              0.0|       0.0|\n",
      "|              0.0|       0.0|\n",
      "|              0.0|       0.0|\n",
      "|              0.0|       0.0|\n",
      "|              1.0|       1.0|\n",
      "|              0.0|       0.0|\n",
      "|              0.0|       0.0|\n",
      "|              1.0|       1.0|\n",
      "+-----------------+----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(test_data)\n",
    "prediction.select('sentiment_indexed', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efbabe9a-19e7-4ea3-b696-671d6fbf7968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d093239-6849-4919-bddd-c37ea81318d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eva = BinaryClassificationEvaluator(labelCol='sentiment_indexed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1ebd4f-8a78-4de1-91e5-8716cc51d4dc",
   "metadata": {},
   "source": [
    "## NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12933a9f-99e0-4b9a-8a44-8eca2c782ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a124cd8c-503b-482d-b1c3-d8abae31fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes(featuresCol='features', labelCol='sentiment_indexed', modelType='multinomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4dfd6d94-7996-4222-93ee-ca7b123261e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_nb = Pipeline(stages=[reg_token, stop_remove, count_vec, idf, pos_neg, clean, nb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73ca21a9-443b-438c-8bdf-a45a824a049c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/27 11:04:11 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "25/08/27 11:04:15 WARN DAGScheduler: Broadcasting large task binary with size 6.1 MiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "model_nb = pipeline_nb.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "895e96c9-0653-449e-953e-2a7de9c7ecb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/27 11:05:16 WARN DAGScheduler: Broadcasting large task binary with size 7.4 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+\n",
      "|sentiment_indexed|prediction|\n",
      "+-----------------+----------+\n",
      "|              1.0|       1.0|\n",
      "|              1.0|       0.0|\n",
      "|              0.0|       0.0|\n",
      "|              0.0|       0.0|\n",
      "|              0.0|       0.0|\n",
      "|              0.0|       0.0|\n",
      "|              1.0|       1.0|\n",
      "|              0.0|       0.0|\n",
      "|              0.0|       0.0|\n",
      "|              1.0|       0.0|\n",
      "+-----------------+----------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "prediction_nb = model_nb.transform(test_data)\n",
    "prediction_nb.select('sentiment_indexed', 'prediction').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6425cc01-9fb7-4cdc-97e8-6f2bc50844a0",
   "metadata": {},
   "source": [
    "## What is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7597c87-acc4-40ca-a7e2-00f8b9bdec8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/27 11:06:37 WARN DAGScheduler: Broadcasting large task binary with size 6.8 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of Logistic Regression: 0.9248127071645409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/27 11:06:41 WARN DAGScheduler: Broadcasting large task binary with size 6.8 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision of Naive Bayes: 0.9248081839603246\n"
     ]
    }
   ],
   "source": [
    "print(f'Precision of Logistic Regression: {eva.evaluate(prediction)}')\n",
    "print(f'Precision of Naive Bayes: {eva.evaluate(prediction)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
