{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4fd6d04-26c9-4b0f-ab72-3ea731a4c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/gerardo-rodriguez/spark-4.0.0-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a720952-85e0-4ce2-8707-68bc98f75888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode\n",
    "spark = SparkSession.builder.appName('str').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c623b09-40e4-40e3-9442-5739f281a689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, when, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3978f79b-815c-4302-8b6e-1c6d5f592701",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user = spark.read.csv('../Create_ratings/netflix_users.csv', header=True, inferSchema=True)\n",
    "schema = df_user.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bce8b820-ff0e-41f0-9e16-76c8ebc22a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/27 12:54:02 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "raw_stream = spark.readStream.format('socket').option('host', 'localhost' ).option('port', 5555).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cf07cb2-5343-42f2-99ef-243d51acba79",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_stream = raw_stream.withColumn(\"tokens\", split(col(\"value\"), \",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc1d7289-f97d-4f13-883a-1c50365b1bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stream = parsed_stream.select(\n",
    "    col(\"tokens\").getItem(0).cast(\"int\").alias(\"Age\"),\n",
    "    col(\"tokens\").getItem(1).cast(\"double\").alias(\"Watch_Time_Hours\"),\n",
    "    col(\"tokens\").getItem(2).cast(\"int\").alias(\"days_login\"),\n",
    "    col(\"tokens\").getItem(3).cast(\"int\").alias(\"subscription_indexed\"),\n",
    "    col(\"tokens\").getItem(4).cast(\"int\").alias(\"country_indexed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d25a8f0-b100-4693-bddd-7974dfb34f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Subscription_Type: string (nullable = true)\n",
      " |-- Watch_Time_Hours: double (nullable = true)\n",
      " |-- Favorite_Genre: string (nullable = true)\n",
      " |-- Last_Login: date (nullable = true)\n",
      "\n",
      "+-------+--------------+---+-------+-----------------+----------------+--------------+----------+-----+----------+\n",
      "|User_ID|          Name|Age|Country|Subscription_Type|Watch_Time_Hours|Favorite_Genre|Last_Login|churn|days_login|\n",
      "+-------+--------------+---+-------+-----------------+----------------+--------------+----------+-----+----------+\n",
      "|      1|James Martinez| 18| France|          Premium|           80.26|         Drama|2024-05-12|    1|       472|\n",
      "|      2|   John Miller| 23|    USA|          Premium|          321.75|        Sci-Fi|2025-02-05|    0|       203|\n",
      "|      3|    Emma Davis| 60|     UK|            Basic|           35.89|        Comedy|2025-01-24|    1|       215|\n",
      "|      4|   Emma Miller| 44|    USA|          Premium|          261.56|   Documentary|2024-03-25|    1|       520|\n",
      "|      5|    Jane Smith| 68|    USA|         Standard|           909.3|         Drama|2025-01-14|    0|       225|\n",
      "+-------+--------------+---+-------+-----------------+----------------+--------------+----------+-----+----------+\n",
      "only showing top 5 rows\n",
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Subscription_Type: string (nullable = true)\n",
      " |-- Watch_Time_Hours: double (nullable = true)\n",
      " |-- Favorite_Genre: string (nullable = true)\n",
      " |-- Last_Login: date (nullable = true)\n",
      " |-- churn: integer (nullable = false)\n",
      " |-- days_login: integer (nullable = true)\n",
      " |-- subscription_indexed: double (nullable = false)\n",
      " |-- country_indexed: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-----+----------+\n",
      "|churn|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "Evaluation Logistic regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/27 13:07:27 WARN DAGScheduler: Broadcasting large task binary with size 1056.1 KiB\n",
      "25/08/27 13:07:28 WARN DAGScheduler: Broadcasting large task binary with size 1535.9 KiB\n",
      "25/08/27 13:07:28 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "25/08/27 13:07:29 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/08/27 13:07:30 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "25/08/27 13:07:31 WARN DAGScheduler: Broadcasting large task binary with size 4.2 MiB\n",
      "25/08/27 13:07:33 WARN DAGScheduler: Broadcasting large task binary with size 4.9 MiB\n",
      "25/08/27 13:07:34 WARN DAGScheduler: Broadcasting large task binary with size 5.4 MiB\n",
      "25/08/27 13:07:35 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/08/27 13:07:36 WARN DAGScheduler: Broadcasting large task binary with size 5.8 MiB\n",
      "25/08/27 13:07:37 WARN DAGScheduler: Broadcasting large task binary with size 5.3 MiB\n",
      "25/08/27 13:07:38 WARN DAGScheduler: Broadcasting large task binary with size 4.3 MiB\n",
      "25/08/27 13:07:38 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/08/27 13:07:39 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/08/27 13:07:40 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|churn|prediction|\n",
      "+-----+----------+\n",
      "|    0|       0.0|\n",
      "|    1|       1.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "|    0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/27 13:07:41 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/08/27 13:07:42 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/08/27 13:07:42 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n",
      "25/08/27 13:07:43 WARN DAGScheduler: Broadcasting large task binary with size 4.5 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas Random Forest\n",
      "Accuracy : 0.9954\n",
      "Precision: 0.9955\n",
      "Recall   : 0.9954\n",
      "F1 Score : 0.9954\n",
      "root\n",
      " |-- show_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- director: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- date_added: string (nullable = true)\n",
      " |-- release_year: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- listed_in: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- rate: double (nullable = false)\n",
      "\n",
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- director: string (nullable = true)\n",
      " |-- release_year: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- rate: double (nullable = false)\n",
      "\n",
      "+--------------------+--------------------+-------------------+\n",
      "|title               |rate                |prediction         |\n",
      "+--------------------+--------------------+-------------------+\n",
      "|Dick Johnson Is Dead|0.4148459087761638  |0.49639538187827437|\n",
      "|Sankofa             |0.907572937611436   |0.4966184202692306 |\n",
      "|The Starling        |0.010386063673253898|0.49637611194353404|\n",
      "|Je Suis Karl        |0.6336888609738293  |0.49649809639107273|\n",
      "|Jeans               |0.8106099316687554  |0.49658098565504555|\n",
      "|Grown Ups           |0.8407089130003395  |0.4964964005227644 |\n",
      "|Dark Skies          |0.5467350941495717  |0.496467014495349  |\n",
      "|Paranoia            |0.15288288553666818 |0.49648547532045684|\n",
      "|Birth of the Dragon |0.8517967132642799  |0.4964261649012684 |\n",
      "|Jaws                |0.36176785099653097 |0.4966631871297266 |\n",
      "+--------------------+--------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "Métricas de Evaluación:\n",
      "RMSE: 0.2887\n",
      "MAE:  0.2495\n",
      "R²:   0.0000\n",
      "root\n",
      " |-- User_ID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Subscription_Type: string (nullable = true)\n",
      " |-- Watch_Time_Hours: double (nullable = true)\n",
      " |-- Favorite_Genre: string (nullable = true)\n",
      " |-- Last_Login: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ClusteringEvaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/ipykernel_28290/4240575661.py:8\u001b[39m\n\u001b[32m      6\u001b[39m model = pipeline_km.fit(df_users)\n\u001b[32m      7\u001b[39m predic = model.transform(df_users)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m eva_clu = \u001b[43mClusteringEvaluator\u001b[49m(featuresCol=\u001b[33m'\u001b[39m\u001b[33mfeatures_scale\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mprediction with k = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(eva_clu.evaluate(predic))\n",
      "\u001b[31mNameError\u001b[39m: name 'ClusteringEvaluator' is not defined"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ClusteringEvaluator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrun\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../Machine_Learning/clasification_logistic_clusterting.ipynb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark.env/lib/python3.13/site-packages/IPython/core/interactiveshell.py:2504\u001b[39m, in \u001b[36mInteractiveShell.run_line_magic\u001b[39m\u001b[34m(self, magic_name, line, _stack_depth)\u001b[39m\n\u001b[32m   2502\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mlocal_ns\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.get_local_scope(stack_depth)\n\u001b[32m   2503\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m-> \u001b[39m\u001b[32m2504\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2506\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2507\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2508\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2509\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark.env/lib/python3.13/site-packages/IPython/core/magics/execution.py:749\u001b[39m, in \u001b[36mExecutionMagics.run\u001b[39m\u001b[34m(self, parameter_s, runner, file_finder)\u001b[39m\n\u001b[32m    747\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m preserve_keys(\u001b[38;5;28mself\u001b[39m.shell.user_ns, \u001b[33m'\u001b[39m\u001b[33m__file__\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    748\u001b[39m         \u001b[38;5;28mself\u001b[39m.shell.user_ns[\u001b[33m'\u001b[39m\u001b[33m__file__\u001b[39m\u001b[33m'\u001b[39m] = filename\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshell\u001b[49m\u001b[43m.\u001b[49m\u001b[43msafe_execfile_ipy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    752\u001b[39m \u001b[38;5;66;03m# Control the response to exit() calls made by the script being run\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark.env/lib/python3.13/site-packages/IPython/core/interactiveshell.py:2995\u001b[39m, in \u001b[36mInteractiveShell.safe_execfile_ipy\u001b[39m\u001b[34m(self, fname, shell_futures, raise_exceptions)\u001b[39m\n\u001b[32m   2993\u001b[39m result = \u001b[38;5;28mself\u001b[39m.run_cell(cell, silent=\u001b[38;5;28;01mTrue\u001b[39;00m, shell_futures=shell_futures)\n\u001b[32m   2994\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m raise_exceptions:\n\u001b[32m-> \u001b[39m\u001b[32m2995\u001b[39m     \u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2996\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result.success:\n\u001b[32m   2997\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark.env/lib/python3.13/site-packages/IPython/core/interactiveshell.py:326\u001b[39m, in \u001b[36mExecutionResult.raise_error\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m.error_before_exec\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.error_in_exec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m.error_in_exec\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/ipykernel_28290/4240575661.py:8\u001b[39m\n\u001b[32m      6\u001b[39m model = pipeline_km.fit(df_users)\n\u001b[32m      7\u001b[39m predic = model.transform(df_users)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m eva_clu = \u001b[43mClusteringEvaluator\u001b[49m(featuresCol=\u001b[33m'\u001b[39m\u001b[33mfeatures_scale\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mprediction with k = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(eva_clu.evaluate(predic))\n",
      "\u001b[31mNameError\u001b[39m: name 'ClusteringEvaluator' is not defined"
     ]
    }
   ],
   "source": [
    "%run ../Machine_Learning/clasification_logistic_clusterting.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6d62404-4efd-48f5-be04-4b8fa0886bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7016e25b-6527-4136-8826-56c94d2bf49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_assembler = VectorAssembler(inputCols=['Age', 'Watch_Time_Hours', 'days_login', 'subscription_indexed', 'country_indexed'], outputCol='features')\n",
    "df_stream_assembled = stream_assembler.transform(df_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fdd3b481-4a73-4c38-baf0-230b27553383",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_stream = model_log.transform(df_stream_assembled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bab4ebf-558a-4634-a0b1-994dec5052ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/27 13:10:41 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6652cd1a-57dd-40d8-b3de-195066a0090a. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/08/27 13:10:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---+----------------+----------+\n",
      "|Age|Watch_Time_Hours|prediction|\n",
      "+---+----------------+----------+\n",
      "+---+----------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---+----------------+----------+\n",
      "|Age|Watch_Time_Hours|prediction|\n",
      "+---+----------------+----------+\n",
      "| 23|           120.0|       1.0|\n",
      "+---+----------------+----------+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+---+----------------+----------+\n",
      "|Age|Watch_Time_Hours|prediction|\n",
      "+---+----------------+----------+\n",
      "| 17|           300.0|       0.0|\n",
      "+---+----------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/27 13:11:48 WARN TextSocketMicroBatchStream: Stream closed by localhost:5555\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gerardo-rodriguez/spark-4.0.0-bin-hadoop3/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/gerardo-rodriguez/spark-4.0.0-bin-hadoop3/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py\", line 535, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/usr/lib/python3.13/socket.py\", line 719, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ~~~~~~~~~~~~~~~~~~~~^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m query = prediction_stream.select(\u001b[33m\"\u001b[39m\u001b[33mAge\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mWatch_Time_Hours\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m).writeStream.outputMode(\u001b[33m'\u001b[39m\u001b[33mappend\u001b[39m\u001b[33m'\u001b[39m).format(\u001b[33m'\u001b[39m\u001b[33mconsole\u001b[39m\u001b[33m'\u001b[39m).start()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark-4.0.0-bin-hadoop3/python/pyspark/sql/streaming/query.py:225\u001b[39m, in \u001b[36mStreamingQuery.awaitTermination\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jsq.awaitTermination(\u001b[38;5;28mint\u001b[39m(timeout * \u001b[32m1000\u001b[39m))\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark-4.0.0-bin-hadoop3/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1361\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1354\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m return_value = get_return_value(\n\u001b[32m   1363\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark-4.0.0-bin-hadoop3/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/spark-4.0.0-bin-hadoop3/python/lib/py4j-0.10.9.9-src.zip/py4j/clientserver.py:535\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m         answer = smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:-\u001b[32m1\u001b[39m])\n\u001b[32m    536\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    537\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    538\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "query = prediction_stream.select(\"Age\", \"Watch_Time_Hours\", \"prediction\").writeStream.outputMode('append').format('console').start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb18fb8-c4d8-4095-a5c2-1b667524e68d",
   "metadata": {},
   "source": [
    "# Explicación académica\n",
    "\n",
    "Spark Streaming permite procesar datos en micro-batches, lo que lo hace escalable y tolerante a fallos.\n",
    "\n",
    "El caso demuestra cómo integrar un modelo ML entrenado previamente (churn prediction) dentro de un flujo de datos en vivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c5707-cfff-47c2-99da-a44ddc5f0831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
